---
permalink: /
title: 
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
 
feature_row1:
  - image_path: c2v_demo.png
    alt: "https://www.code2vec.org"
    title: "code2vec (POPL'2019)"
    url: "https://www.code2vec.org"
    btn_label: "Demo"
    btn_class: "btn--primary"
    url2: "https://github.com/tech-srl/code2vec"
    btn_label2: "Code"
    btn_class: "btn--primary"
    tags: 
        - code2vec
        
feature_row2:
  - image_path: c2s_demo.png
    alt: "https://www.code2seq.org"
    title: "code2seq (ICLR'2019)"
    url: "https://www.code2seq.org"
    btn_label: "Demo"
    btn_class: "btn--primary"
    url2: "https://github.com/tech-srl/code2seq"
    btn_label2: "Code"
    btn_class: "btn--primary"
    tags: 
        - code2seq

feature_row3:
  - image_path: slm_demo.png
    alt: "https://www.AnyCodeGen.org"
    title: "AnyCodeGen (ICML'2020)"
    url: "https://www.AnyCodeGen.org"
    btn_label: "Demo"
    btn_class: "btn--primary"
    url2: "https://github.com/tech-srl/slm-code-generation"
    btn_label2: "Code"
    btn_class: "btn--primary"
    tags: 
        - AnyCodeGen

---

I am a postdocoral researcher at the Language Technologies Institute (LTI) of Carnegie Mellon University, working with [Prof. Graham Neubig](https://www.phontron.com/){:target="_blank"}, a member of [NeuLab](https://www.cs.cmu.edu/~neulab/){:target="_blank"}, and a [Rothschild Fellow](https://issuu.com/dleventer/docs/fellows-2021_booklet_corr-4/12){:target="_blank"}.

<!-- My research interests are broad and include programming language processing (PLP), natural language processing (NLP) and deep learning in general. -->

I obtained my PhD in the department of Computer Science at the Technion, where I was fortunate to be advised by [Prof. Eran Yahav](https://www.cs.technion.ac.il/~yahave/){:target="_blank"}. My PhD dissertation has been awarded the [Reynolds Doctoral Dissertation Award](https://www.sigplan.org/Awards/Dissertation/){:target="_blank"} (formerly "SIGPLAN Outstanding Doctoral Dissertation Award").

Previously, I served 7 years as an officer onboard a missile ship in the Israeli Navy.
Later, I completed my BSc _summa cum laude_ at the Computer Science Department at the Technion, as an alumnus of The Rothschild-Technion Scholars Program for Excellence. 
Between 2014-2016, I worked at Microsoft R&D center in Haifa, developing data security services for the cloud. 
Between June-September of 2018, I interned at Google New-York, researching neural models for speech recognition.

In addition, I hold a B.A. in Humanities.

I am happily married to Lee and father of Gur ðŸ™‚

### News
* **_May 2023_** - A new paper: [On the Expressivity Role of LayerNorm in Transformersâ€™ Attention](#layernorm) was accepted to Findings of the ACL'2023!
* **_May 2023_** - a new preprint: [Unlimiformer: Long-Range Transformers with Unlimited Length Input](#unlimiformer)
* **_April 2022_** - [PAL: Program-aided Language Models](#pal) and [Why do kNN-LMs Work?](#knnwhy) were accepted to ICML'2023! 
* **_April 2023_** - a new preprint: [Self-Refine: Iterative Refinement with Self-Feedback](#selfrefine)
* **_March 2023_** - [Learning Performance-Improving Code Edits](#pie) and [CodeBERTScore](#cbs) (**Spotlight**!) will appear in the [Deep Learning for Code](https://dl4c.github.io/){:target="_blank"} ICLR'2023 workshop
* **_February 2023_** - a new preprint: [Learning Performance-Improving Code Edits](#pie)
* **_February 2023_** - a new preprint: [CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code](#cbs)
* **_January 2023_** - Our [DocPrompting](#doccoder) paper was accepted to ICLR'2023 as a **Spotlight**!
* **_January 2023_** - a new preprint: [Why do Nearest Neighbor Language Models Work?](#knnwhy)
* **_December 2022_** - A [new demo](https://huggingface.co/spaces/JavaFXpert/gpt-math-techniques){:target="_blank"} for [PAL](#pal)!
* **_December 2022_** - I was invited to the [explAInable podcast](https://explainable.podbean.com/e/%d7%99%d7%a6%d7%99%d7%a8%d7%94%d7%90%d7%95%d7%98%d7%95%d7%9e%d7%98%d7%99%d7%aa%d7%a9%d7%9c-%d7%a7%d7%95%d7%93-%d7%a2%d7%9d%d7%a4%d7%a8%d7%95%d7%a4/){:target="_blank"} (Hebrew)
* **_November 2022_** - a new preprint: [PaL: Program-aided Language Models](#pal)
* **_October 2022_** - Our paper [Language Models of Code are Few-Shot Commonsense Learners](#cocogen) was accepted to EMNLP'2022!
* **_September 2022_** - We released a new repository for evaluation of code generation: [code-bert-score](https://github.com/neulab/code-bert-score){:target="_blank"}, along with pretrained models of several programming languages, based on CodeBERT.
* **_August 2022_** - a new preprint: [`DocPrompting`: Generating Code by Retrieving the Docs](#doccoder)
* **_July 2022_** - I released a new HuggingFace ðŸ¤— `transformers` implementation of [RetoMaton](#retomaton), kNN-language models and kNN-machine translation: [https://github.com/neulab/knn-transformers](https://github.com/neulab/knn-transformers){:target="_blank"}
* **_June 2022_** - I was selected for the [ACM SIGPLAN Reynolds Doctoral Dissertation Award](https://www.sigplan.org/Awards/Dissertation/){:target="_blank"} (formerly "SIGPLAN Outstanding Doctoral Dissertation Award")!
* **_May 2022_** - Our [RetoMaton](#retomaton) paper was accepted to ICML'2022!
* **_April 2022_** - Our [PolyCoder](#polycoder) paper will appear in ICLR 2022's [DL4Code](https://dl4c.github.io/papers/){:target="_blank"} and PLDI 2022's [MAPS](https://pldi22.sigplan.org/home/maps-2022){:target="_blank"} workshops.
* **_March 2022_** - A new preprint: [A Systematic Evaluation of Large Language Models of Code](#polycoder)
* **_February 2022_** - A new preprint: [Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval](#retomaton)
* **_January 2022_** - Our paper [How Attentive are Graph Attention Networks?](#attentive) was accepted to ICLR'2022! 

---
# Publications <a href="https://scholar.google.com/citations?user=QBn7vq8AAAAJ&hl=en&oi=ao" target="_blank"><i class="fas fa-fw fa-graduation-cap"></i></a><a name="publications"></a>

## Preprints
* **WebArena: A Realistic Web Environment for Building Autonomous Agents**{:id="webarena"}
    * Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, **Uri Alon**, Graham Neubig
    * Press: [[MarkTechPost]](https://www.marktechpost.com/2023/07/27/cmu-researchers-introduce-webarena-a-realistic-and-reproducible-web-environment-with-4-real-world-web-apps-for-benchmarking-useful-agents/){:target="_blank"} [[TS2]](https://ts2.space/en/training-autonomous-agents-new-research-introduces-webarena/){:target="_blank"} [[Towards AI]](https://pub.towardsai.net/unlimiformer-long-range-transformers-with-unlimited-length-input-3725f69b0d03){:target="_blank"} 
    * [[PDF]](https://arxiv.org/pdf/2307.13854.pdf){:target="_blank"} 
    [[Code]](https://github.com/web-arena-x/webarena){:target="_blank"} [[Website]](https://webarena.dev){:target="_blank"}
    [[Tweet]](https://twitter.com/shuyanzhxyc/status/1683917253597855744?s=20){:target="_blank"} 
    [[BibTex]](https://pastebin.com/raw/Uf0pup43){:target="_blank"}
* **Unlimiformer: Long-Range Transformers with Unlimited Length Input**{:id="unlimiformer"}
    * Amanda Bertsch, **Uri Alon**, Graham Neubig, Matthew R. Gormley
    * Press: [[Synched]](https://syncedreview.com/2023/05/04/cmus-unlimiformer-augments-transformers-to-enable-unbounded-input-lengths/){:target="_blank"} [[MarkTechPost]](https://www.marktechpost.com/2023/05/07/cmu-researchers-introduce-unlimiformer-an-ai-method-for-augmenting-pretrained-encoder-decoders-with-an-external-datastore-to-allow-for-unlimited-length-input/){:target="_blank"} [[Medium]](https://medium.com/techciting/techciting-issue-2-may-7-2023-c3b02e606068){:target="_blank"} [[Towards AI]](https://pub.towardsai.net/unlimiformer-long-range-transformers-with-unlimited-length-input-3725f69b0d03){:target="_blank"} 
    * [[PDF]](https://arxiv.org/pdf/2305.01625.pdf){:target="_blank"} 
    [[Code]](https://github.com/abertsch72/unlimiformer){:target="_blank"}
    [[Tweet]](https://twitter.com/abertsch72/status/1654110919977324545?s=20){:target="_blank"} 
    [[BibTex]](https://pastebin.com/raw/tHgJ6AhF){:target="_blank"}
* **Self-Refine: Iterative Refinement with Self-Feedback**{:id="selfrefine"}
    * Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, **Uri Alon**, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, Peter Clark
    * Online demo: [https://self-refine-webgen.herokuapp.com/](https://self-refine-webgen.herokuapp.com/){:target="_blank"}
    * Press: [[MarkTechPost]](https://www.marktechpost.com/2023/04/07/this-ai-paper-introduce-self-refine-a-framework-for-improving-initial-outputs-from-llms-through-iterative-feedback-and-refinement/){:target="_blank"} [[Medium]](https://kargarisaac.medium.com/self-refine-a-new-milestone-in-the-ai-era-ac51568873da?source=topics_v2---------5-84--------------------fda4d9ef_ad4f_488d_9028_248a261e661a-------17){:target="_blank"} [[EmergentMind]](https://www.emergentmind.com/posts/self-refine-iterative-refinement-with-self-feedback){:target="_blank"} 
    * [[PDF]](https://arxiv.org/pdf/2303.17651.pdf){:target="_blank"} 
    [[Code]](https://github.com/madaan/self-refine){:target="_blank"} [[Website]](https://selfrefine.info){:target="_blank"} [[Tweet]](https://twitter.com/johnjnay/status/1642704826776559617?s=20){:target="_blank"} [[BibTex]](https://pastebin.com/raw/stKd6rEB){:target="_blank"}

* **Learning Performance-Improving Code Edits**{:id="pie"}
    * Aman Madaan, Alexander Shypula, **Uri Alon**, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, Amir Yazdanbakhsh
    * To appear in [Deep Learning for Code](https://dl4c.github.io/){:target="_blank"}, ICLR'2023 workshop    
    * [[PDF]](https://arxiv.org/pdf/2302.07867.pdf){:target="_blank"} [[Code]](https://github.com/madaan/pie-perf){:target="_blank"} [[Website]](https://pie4perf.com/){:target="_blank"} [[BibTex]](https://pastebin.com/raw/WasRirHS){:target="_blank"}

* **CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code**{:id="cbs"}
    * Shuyan Zhou, **Uri Alon**, Sumit Agarwal, Graham Neubig  
    * To appear in [Deep Learning for Code](https://dl4c.github.io/){:target="_blank"}, ICLR'2023 workshop (<span style="color:red">**Spotlight**</span>) 
    * Press: [[Non-Brand Data]](https://cornellius.substack.com/p/automatic-evaluation-metric-for-code){:target="_blank"}
    * [[PDF]](https://arxiv.org/pdf/2302.05527.pdf){:target="_blank"} [[Code]](https://github.com/neulab/code-bert-score){:target="_blank"} [[Huggingface Models]](https://github.com/neulab/code-bert-score#backend-model) [[BibTex]](https://pastebin.com/raw/ugjgw1XM){:target="_blank"}


## Accepted Papers
* **On the Expressivity Role of LayerNorm in Transformersâ€™ Attention**{:id="layernorm"}
    * Shaked Brody, **Uri Alon**, Eran Yahav
    * To appear in **_Findings of ACL'2023_**
    * Press: [[Medium]](https://lessw.medium.com/what-layernorm-really-does-for-attention-in-transformers-4901ea6d890e#:~:text=A%20%E2%80%94%20Projection%3A%20LayerNorm%20helps%20the,be%20orthogonal%20to%20the%20queries){:target="_blank"}
    * [[PDF]](https://arxiv.org/pdf/2305.02582){:target="_blank"} [[Code]](https://t.co/cfIEOT8fh3){:target="_blank"} [[Tweet]](https://twitter.com/shakedbr/status/1654449044851159040?s=20){:target="_blank"} 
    [[BibTex]](https://pastebin.com/raw/xFYce3Vt){:target="_blank"}
* **Why do Nearest Neighbor Language Models Work?**{:id="knnwhy"}
    * Frank F. Xu, **Uri Alon**, Graham Neubig  
    * To appear in **_ICML'2023_**
    * [[PDF]](https://arxiv.org/pdf/2301.02828.pdf){:target="_blank"} [[Code]](https://github.com/frankxu2004/knnlm-why){:target="_blank"} [[Tweet]](https://twitter.com/urialon1/status/1612817186678083585?s=20&t=srvqJDMrLcB1_Zzdm0-5pA){:target="_blank"} [[BibTex]](https://pastebin.com/raw/UeDvMh6X){:target="_blank"}

* **PAL: Program-aided Language Models**{:id="pal"}
    * Luyu Gao, Aman Madaan, Shuyan Zhou, **Uri Alon**, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig
    * To appear in **_ICML'2023_**
    * Press: [[AI Trends 2023 Podcast]](https://twimlai.com/podcast/twimlai/ai-trends-2023-natural-language-proc-chatgpt-gpt-4-and-cutting-edge-research/){:target="_blank"} [[Medium]](https://cobusgreyling.medium.com/pal-program-aided-large-language-models-30db3e59f796){:target="_blank"} 
    * Online demo: [https://huggingface.co/spaces/JavaFXpert/gpt-math-techniques](https://huggingface.co/spaces/JavaFXpert/gpt-math-techniques){:target="_blank"}
    * [[PDF]](https://arxiv.org/pdf/2211.10435.pdf){:target="_blank"} 
    [[Code]](https://github.com/reasoning-machines/pal/){:target="_blank"} [[Website]](http://reasonwithpal.com/){:target="_blank"} [[Tweet]](https://twitter.com/urialon1/status/1594714720468144129?s=20&t=vbFfTseYXTRMbKtftRLjPA){:target="_blank"} [[BibTex]](https://pastebin.com/raw/76dwPn3V){:target="_blank"}

* **`DocPrompting`: Generating Code by Retrieving the Docs**{:id="doccoder"}
    * Shuyan Zhou
, **Uri Alon**, Frank F. Xu, Zhengbao Jiang, Graham Neubig    
    * Appeared in **_ICLR'2023_** (<span style="color:red">**Spotlight**</span>)
    * Press: [[MarkTechPost]](https://www.marktechpost.com/2023/02/25/cmu-researchers-propose-docprompting-a-natural-language-to-code-generation-approach-by-retrieving-code-documentation/){:target="_blank"} [[Medium]](https://medium.com/syncedreview/cmu-inspired-cognitions-docprompting-improves-code-generation-by-retrieving-relevant-4bc72992130d){:target="_blank"} [[Prophet-Con]](https://prophetcon.com/2023/02/25/cmu-researchers-propose-docprompting-a-natural-language-to-code-generation-approach-by-retrieving-code-documentation/){:target="_blank"} [[Synched]](https://syncedreview.com/2023/02/28/cmu-inspired-cognitions-docprompting-improves-code-generation-by-retrieving-relevant-documentation/){:target="_blank"}
    * [[PDF]](https://arxiv.org/pdf/2207.05987.pdf){:target="_blank"} [[Code]](https://github.com/shuyanzhou/doccoder){:target="_blank"} [[BibTex]](https://pastebin.com/raw/Jes96jqS){:target="_blank"}


* **Language Models of Code are Few-Shot Commonsense Learners**{:id="cocogen"}
    * Aman Madaan, Shuyan Zhou, **Uri Alon**, Yiming Yang, Graham Neubig
    * Appeared in **_EMNLP'2022_**
    * [[PDF]](https://arxiv.org/pdf/2210.07128.pdf){:target="_blank"} [[Code]](https://github.com/madaan/CoCoGen){:target="_blank"} [[Tweet]](https://twitter.com/urialon1/status/1583100368237449216?s=20&t=vbFfTseYXTRMbKtftRLjPA){:target="_blank"} [[BibTex]](https://pastebin.com/raw/bAQW3kuC){:target="_blank"}

* **Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval**{:id="retomaton"} (RetoMaton)
    * **Uri Alon**, Frank F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, Graham Neubig
    * Appeared in **_ICML'2022_**
    * [[PDF]](https://arxiv.org/pdf/2201.12431){:target="_blank"} [[Poster]](files/Retomaton_poster_ICML_2022.pdf){:target="_blank"} [[5-min Video]](https://slideslive.com/38983799/neurosymbolic-language-modeling-with-retrieval-automaton){:target="_blank"} [[1-hour Video]](https://www.youtube.com/watch?v=-Au42BuWTEc){:target="_blank"} [[Slides]](files/retomaton_talk.pdf){:target="_blank"}  [[Tweet]](https://twitter.com/urialon1/status/1532371214869708801?s=20&t=vbFfTseYXTRMbKtftRLjPA){:target="_blank"}  [[BibTex]](https://pastebin.com/raw/9J5QUDPX){:target="_blank"}
    * [[Code - `fairseq` implementation]](https://github.com/neulab/retomaton){:target="_blank"} 
    * [[Code - HuggingFace ðŸ¤— `transformers` implementation]](https://github.com/neulab/knn-transformers){:target="_blank"} [[Trained models]](https://github.com/neulab/knn-transformers#available-models){:target="_blank"} 

* **How Attentive are Graph Attention Networks?**{:id="attentive"}
    * Shaked Brody, **Uri Alon**, Eran Yahav
    * Appeared in **_ICLR'2022_**
    * [[PDF]](https://arxiv.org/pdf/2105.14491.pdf){:target="_blank"} [[Poster]](https://shakedbr.cswp.cs.technion.ac.il/wp-content/uploads/sites/112/2022/03/poster.001.png){:target="_blank"} [[Code]](https://github.com/tech-srl/how_attentive_are_gats){:target="_blank"} [[Video]](https://drive.google.com/file/d/1B2uSH66dfROAgK7RVURjC4WrdSx9tPS_/view){:target="_blank"} [[BibTex]](https://pastebin.com/raw/besiLuqp){:target="_blank"}
    * GATv2 implementations:
      * [[PyTorch Geometric]](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATv2Conv){:target="_blank"}: `from torch_geometric.nn.conv.gatv2_conv import GATv2Conv`
      * [[DGL]](https://docs.dgl.ai/en/latest/api/python/nn.pytorch.html#gatv2conv): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`from dgl.nn.pytorch import GATv2Conv`
      * [[TensorFlow GNN]](https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/api_docs/python/gnn/keras/layers/GATv2.md){:target="_blank"}: &nbsp;&nbsp;&nbsp;`from tensorflow_gnn.keras.layers import GATv2`  

* **On the Bottleneck of Graph Neural Networks and its Practical Implications**
    * **Uri Alon**, Eran Yahav
    * Appeared in **_ICLR'2021_**
    * [[PDF]](https://arxiv.org/pdf/2006.05205){:target="_blank"} [[Poster]](files/bottleneck_poster.pdf){:target="_blank"} [[Slides]](files/bottleneck_slides.pdf){:target="_blank"} [**[Video]**](https://youtu.be/vrLsEwzZTCQ){:target="_blank"} [[Code]](https://github.com/tech-srl/bottleneck/){:target="_blank"} [[BibTex]](https://pastebin.pl/view/raw/f940b575){:target="_blank"}

* **A Structural Model for Contextual Code Changes**
    * Shaked Brody, **Uri Alon**, Eran Yahav
    * Appeared in **_OOPSLAâ€™2020_**
    * [[PDF]](https://arxiv.org/pdf/2005.13209.pdf){:target="_blank"} [[Video]](https://youtu.be/NgY6nx3bU0M){:target="_blank"} [[Code]](https://github.com/tech-srl/c3po){:target="_blank"} [[BibTex]](https://pastebin.pl/view/raw/71d7c823){:target="_blank"}

* **Adversarial Examples for Models of Code**
    * Noam Yefet, **Uri Alon**, Eran Yahav
    * Appeared in **_OOPSLAâ€™2020_**
    * [[PDF]](https://arxiv.org/pdf/1910.07517.pdf){:target="_blank"} [[Video]](https://youtu.be/lOdXDxaUkII){:target="_blank"} [[Code]](https://github.com/tech-srl/adversarial-examples){:target="_blank"} [[BibTex]](https://pastebin.pl/view/raw/3c8512de){:target="_blank"}

* **Neural Reverse Engineering of Stripped Binaries using Augmented Control Flow Graphs**
    * Yaniv David, Uri Alon, Eran Yahav
    * Appeared in **_OOPSLAâ€™2020_**
    * [[PDF]](https://arxiv.org/pdf/1902.09122){:target="_blank"} [[Video]](https://youtu.be/CnCYd9QuiJ0){:target="_blank"} [[Code]](https://github.com/tech-srl/Nero){:target="_blank"} [[BibTex]](https://pastebin.pl/view/raw/d8794bd3){:target="_blank"}

* **Structural Language Models of Code**
    * **Uri Alon**, Roy Sadaka, Omer Levy, Eran Yahav
    * Appeared in **_ICMLâ€™2020_**
    * Online demo: [https://www.AnyCodeGen.org](https://www.AnyCodeGen.org){:target="_blank"}
    * [[PDF]](https://arxiv.org/pdf/1910.00577.pdf){:target="_blank"} [[Poster]](files/slm_poster.pdf){:target="_blank"} [[Slides]](files/slm-icml.pdf){:target="_blank"} [[Video]](https://slideslive.com/38927682/structural-language-models-of-code){:target="_blank"} [[Data]](https://github.com/tech-srl/slm-code-generation){:target="_blank"} [[Blog]](https://blog.sigplan.org/2020/05/11/from-programs-to-deep-models-part-3-code-completion/){:target="_blank"} [[BibTex]](https://pastebin.com/raw/8D64vzux){:target="_blank"}

* **Contextual Speech Recognition with Difficult Negative Training Examples**
    * **Uri Alon**, Golan Pundak, Tara N. Sainath
    * Appeared in **_ICASSPâ€™2019_**
    * [[PDF]](https://arxiv.org/pdf/1810.12170){:target="_blank"} [[Poster]](files/icassp2019_poster.pdf){:target="_blank"} [[BibTex]](https://pastebin.com/raw/yfRdBvS1){:target="_blank"}
    
* **code2seq: Generating Sequences from Structured Representations of Code**
    * **Uri Alon**, Shaked Brody, Omer Levy, Eran Yahav
    * Appeared in **_ICLRâ€™2019_**
    * Online demo: [https://code2seq.org](https://code2seq.org){:target="_blank"}
    * [[PDF]](https://arxiv.org/pdf/1808.01400){:target="_blank"} [[Poster]](files/ICLR19_poster_code2seq.pdf){:target="_blank"} [[Blog]](https://blog.sigplan.org/2020/02/12/from-programs-to-deep-models-part-2/){:target="_blank"} [[Code]](https://github.com/tech-srl/code2seq/){:target="_blank"} [[BibTex]](https://pastebin.com/raw/bw548FeG){:target="_blank"}

* **code2vec: Learning Distributed Representations of Code**
    * **Uri Alon**, Meital Zilberstein, Omer Levy, Eran Yahav
    * Appeared in **_POPLâ€™2019_**
    * [**ACM SIGPLAN Research Highlight**](https://www.sigplan.org/Highlights/Papers/){:target="_blank"}
    * Online demo: [https://www.code2vec.org](https://www.code2vec.org){:target="_blank"}
    * [[PDF]](https://arxiv.org/pdf/1803.09473){:target="_blank"} [[Slides (PDF)]](files/code2vec_popl19_slides.pdf){:target="_blank"} [[Slides (PPT)]](files/code2vec_popl19_presentation.pptx){:target="_blank"} [[Video]](https://t.co/fjz2oVoDus){:target="_blank"} [[Blog]](https://blog.sigplan.org/2020/02/12/from-programs-to-deep-models-part-2/){:target="_blank"} [[Code]](https://github.com/tech-srl/code2vec){:target="_blank"} [[BibTex]](https://pastebin.com/raw/ddAkvska){:target="_blank"}

* **A General Path-Based Representation for Predicting Program Properties**
    * **Uri Alon**, Meital Zilberstein, Omer Levy, Eran Yahav
    * Appeared in **_PLDIâ€™2018_**
    * [[PDF]](https://arxiv.org/pdf/1803.09544.pdf){:target="_blank"} [[Slides]](files/pldi18_slides.pdf){:target="_blank"} [[Video]](https://urialon.cswp.cs.technion.ac.il/wp-content/uploads/sites/83/2021/w03/bottleneck_poster.pdf){:target="_blank"} [[Blog]](https://blog.sigplan.org/2019/08/22/from-programs-to-deep-models-part-1/){:target="_blank"} [[Code]](https://github.com/tech-srl/PigeonJS){:target="_blank"} [[BibTex]](https://pastebin.com/raw/5tZQJ0ch){:target="_blank"}



## Workshops


* **A Systematic Evaluation of Large Language Models of Code**{:id="polycoder"} (PolyCoder)
    * Frank F. Xu, **Uri Alon**, Graham Neubig, Vincent J. Hellendoorn
    * Appeared in [MAPS'2022](https://pldi22.sigplan.org/details/maps-2022-papers/1/A-Systematic-Evaluation-of-Large-Language-Models-of-Code){:target="_blank"}
    * Appeared in [Deep Learning for Code](https://openreview.net/forum?id=SLcEnoObJZq){:target="_blank"}, ICLR'2022 workshop
    * Press: [[Forbes]](https://www.forbes.com/sites/janakirammsv/2022/03/14/5-ai-tools-that-can-generate-code-to-help-programmers){:target="_blank"} [[ZDNet]](https://www.zdnet.com/article/programming-languages-this-open-source-ai-code-generator-is-very-good-at-writing-in-c/){:target="_blank"} [[VentureBeat]](https://venturebeat.com/2022/03/04/researchers-open-source-code-generating-ai-they-claim-can-beat-openais-codex/){:target="_blank"} [[MarkTechPost]](https://www.marktechpost.com/2022/03/08/cmu-researchers-open-source-polycoder-a-machine-learning-based-code-generator-with-2-7b-parameters/){:target="_blank"}
    * [[PDF]](https://arxiv.org/pdf/2202.13169){:target="_blank"} [[Code]](https://github.com/VHellendoorn/Code-LMs){:target="_blank"} [[BibTex]](https://pastebin.com/raw/ZeNXqKUx){:target="_blank"}
    * HuggingfaceðŸ¤— model: [NinedayWang/PolyCoder-2.7B](https://huggingface.co/NinedayWang/PolyCoder-2.7B){:target="_blank"}

* **Single-Node Attack for Fooling Graph Neural Networks**
    * Ben Finkelshtein, Chaim Baskin, Evgenii Zheltonovzhskii, **Uri Alon**
    * DLG-KDD'2021 [[PDF]](https://arxiv.org/pdf/2011.03574){:target="_blank"} [[Code]](https://github.com/benfinkelshtein/SINGLE){:target="_blank"} [[BibTex]](https://pastebin.pl/view/raw/3f25046a){:target="_blank"}
      

## PhD Thesis
 * **Machine Learning for Programming Language Processing**
     * Computer Science Department, Technion, 2021
     * Awarded the [Reynolds Doctoral Dissertation Award](https://www.sigplan.org/Awards/Dissertation/){:target="_blank"} (formerly "SIGPLAN Outstanding Doctoral Dissertation Award")
     * [[PDF]](files/thesis.pdf){:target="_blank"}

## Technical Reports

* **Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling**
    * Jonathan Shen, ..., **Uri Alon**, ...
    * [[PDF]](https://arxiv.org/pdf/1902.08295.pdf){:target="_blank"}


---
# Awards
* 2022 - [Reynolds Doctoral Dissertation Award](https://www.sigplan.org/Awards/Dissertation/){:target="_blank"} (formerly "SIGPLAN Outstanding Doctoral Dissertation Award")
* 2021-2022 â€“ [Rothschild Post-Doctoral Fellowship](https://issuu.com/dleventer/docs/fellows-2021_booklet_corr-4/12){:target="_blank"}
* 2021-2022 â€“ Fulbright Post-Doctoral Fellowship (declined)
* 2020 â€“ [ACM SIGPLAN Research Highlight](https://www.sigplan.org/Highlights/Papers/){:target="_blank"}, "code2vec: Learning Distributed Representations of Code" (POPLâ€™2019)
* 2019 â€“ Jacobs Excellence Scholarship
* 2019 â€“ Department Funding Excellence Scholarship
* 2018 â€“ Department Funding Excellence Scholarship
* 2016 â€“ Excellent Teaching Assistant
* 2016 â€“ Deanâ€™s Excellent Scholarship
* 2016 â€“ Alumnus of the Rothchild-Technion Program for Excellence
* 2015 â€“ SAMBA â€“ CS Excellent Students

---
# Demos


{% include feature_row id="feature_row3" type="left" %}
<a name="AnyCodeGen"></a> 

{% include feature_row id="feature_row2" type="left" %}
<a name="code2seq"></a> 

{% include feature_row id="feature_row1" type="left" %}
<a name="code2vec"></a> 



---
# Service
* Reviewer: ICLR'2023, NeurIPS '2022 (**Outstanding Reviewer**), TMLR, ICML'2022 (**Outstanding Reviewer - top 10%**), ICLR'2022 (**Highlighted Reviewer**), AIPLANS NeurIPS 2021 workshop, ICMLâ€™2021 (**top 10% Best Reviewers**), ICLRâ€™2021, NeurIPSâ€™2020, ICLRâ€™2020
* Program Committee: MAPS'2022, Deep Learning for Code ICLR'22 workshop, PLDIâ€™2021, NeurIPSâ€™2020 CAP workshop, AIDMâ€™20, AIDMâ€™19
* Area Chair: Learning on Graphs '2022


